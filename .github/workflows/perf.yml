name: Performance smoke benchmark

permissions:
  contents: read
  pull-requests: write

on:
  schedule:
    - cron: '0 6 * * *'
  workflow_dispatch:
    inputs:
      pr-number:
        description: 'Optional pull request number to comment on'
        required: false
      dashboard-endpoint:
        description: 'Optional URL to receive the summarized JSON payload'
        required: false

jobs:
  smoke:
    name: Firewood smoke benchmark
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install nightly toolchain
        uses: dtolnay/rust-toolchain@nightly

      - name: Cache Cargo artifacts
        uses: Swatinem/rust-cache@v2
        with:
          shared-key: smoke-benchmark

      - name: Run smoke benchmark
        env:
          FIREWOOD_SMOKE_OUTPUT: smoke-metrics.json
        run: |
          set -euo pipefail
          cargo +nightly run -p firewood-benchmark -- smoke \
            | tee smoke-benchmark.log

      - name: Summarize metrics
        run: |
          set -euo pipefail
          python3 tools/perf_smoke_summary.py \
            --metrics smoke-metrics.json \
            --summary-md smoke-summary.md \
            --summary-json smoke-summary.json \
            --summary-html smoke-summary.html

      - name: Comment summary on pull request
        if: github.event_name == 'pull_request' || (github.event_name == 'workflow_dispatch' && github.event.inputs.pr-number != '')
        uses: actions/github-script@v7
        env:
          PR_NUMBER: ${{ github.event.pull_request.number || github.event.inputs.pr-number }}
        with:
          script: |
            const fs = require('fs');
            const prNumber = Number(process.env.PR_NUMBER);
            if (!prNumber) {
              core.info('No pull request target provided; skipping summary comment');
              return;
            }

            const summary = fs.readFileSync('smoke-summary.md', 'utf8');
            const body = [
              'Performance smoke benchmark summary:',
              '',
              summary.trim(),
              '',
              'Additional artifacts: `smoke-summary.json` (machine readable) and `smoke-summary.html` (rich view).',
            ].join('\n');

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber,
              body,
            });

      - name: Validate performance Grafana dashboards
        env:
          PERF_GRAFANA_URL: ${{ secrets.PERF_GRAFANA_URL }}
          PERF_GRAFANA_API_KEY: ${{ secrets.PERF_GRAFANA_API_KEY }}
        run: |
          set -euo pipefail
          tools/perf_dashboard_check.sh --manifest docs/performance_dashboards.json

      - name: Publish summary to dashboard endpoint
        if: github.event_name == 'workflow_dispatch' && github.event.inputs.dashboard-endpoint != ''
        env:
          DASHBOARD_ENDPOINT: ${{ github.event.inputs.dashboard-endpoint }}
          DASHBOARD_TOKEN: ${{ secrets.PERF_DASHBOARD_TOKEN }}
        run: |
          set -euo pipefail
          if [ -z "${DASHBOARD_TOKEN:-}" ]; then
            echo "PERF_DASHBOARD_TOKEN is not configured; skipping dashboard upload"
            exit 0
          fi

          curl --fail-with-body \
            -X POST \
            -H "Authorization: Bearer ${DASHBOARD_TOKEN}" \
            -H "Content-Type: application/json" \
            --data-binary @smoke-summary.json \
            "${DASHBOARD_ENDPOINT}"

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: smoke-benchmark
          path: |
            smoke-benchmark.log
            smoke-metrics.json
            smoke-summary.md
            smoke-summary.json
            smoke-summary.html

  pruning:
    name: Pruning benchmark
    runs-on: ubuntu-latest
    strategy:
      matrix:
        backend: [standard, io-uring]
        branch_factor: [16, 256]
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install nightly toolchain
        uses: dtolnay/rust-toolchain@nightly

      - name: Cache Cargo artifacts
        uses: Swatinem/rust-cache@v2
        with:
          shared-key: pruning-benchmark-${{ matrix.backend }}-bf${{ matrix.branch_factor }}

      - name: Run pruning benchmark
        env:
          BACKEND: ${{ matrix.backend }}
          BRANCH_FACTOR: ${{ matrix.branch_factor }}
        run: |
          set -euo pipefail
          FEATURES=()
          if [ "$BACKEND" = "io-uring" ]; then
            FEATURES+=("io-uring")
          fi
          if [ "$BRANCH_FACTOR" = "256" ]; then
            FEATURES+=("branch_factor_256")
          fi

          OUTPUT_PATH="pruning-metrics-${BACKEND}-bf${BRANCH_FACTOR}.json"
          export FIREWOOD_PRUNING_OUTPUT="${OUTPUT_PATH}"

          LOG_PATH="pruning-${BACKEND}-bf${BRANCH_FACTOR}.log"
          if [ ${#FEATURES[@]} -gt 0 ]; then
            cargo +nightly run -p firewood-benchmark --features "${FEATURES[*]}" -- pruning \
              | tee "${LOG_PATH}"
          else
            cargo +nightly run -p firewood-benchmark -- pruning \
              | tee "${LOG_PATH}"
          fi

      - name: Upload pruning artifacts
        uses: actions/upload-artifact@v4
        with:
          name: pruning-benchmark-${{ matrix.backend }}-bf${{ matrix.branch_factor }}
          path: |
            pruning-${{ matrix.backend }}-bf${{ matrix.branch_factor }}.log
            pruning-metrics-${{ matrix.backend }}-bf${{ matrix.branch_factor }}.json

  folding:
    name: Global proof benchmarks
    runs-on: ubuntu-latest
    env:
      FOLDING_PERF_OUTPUT: folding-perf.json
      FOLDING_PERF_CHAIN_LENGTH: 256
      FOLDING_PERF_PROOF_BYTES: 4096
      FOLDING_MAX_AGGREGATED_KIB: 12
      FOLDING_MAX_NOVA_KIB: 16
      FOLDING_MAX_AGGREGATED_VERIFY_MS: 1.5
      FOLDING_MAX_NOVA_VERIFY_MS: 2.5
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install stable toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache Cargo artifacts
        uses: Swatinem/rust-cache@v2
        with:
          shared-key: folding-benchmarks

      - name: Run folding performance sampler
        run: |
          set -euo pipefail
          cargo run -p rpp-chain --bin folding_perf --release \
            --quiet

      - name: Summarize folding metrics
        run: |
          set -euo pipefail
          jq -r '.runs[] | ["version=" + .version, "avg_bytes=" + (.avg_proof_bytes|tostring), "max_bytes=" + (.max_proof_bytes|tostring), "verify_ms=" + (.verify_per_proof_ms|tostring)] | join("\t")' "${FOLDING_PERF_OUTPUT}" > folding-perf-summary.tsv

      - name: Enforce folding thresholds
        run: |
          set -euo pipefail

          agg_limit_bytes=$((FOLDING_MAX_AGGREGATED_KIB * 1024))
          nova_limit_bytes=$((FOLDING_MAX_NOVA_KIB * 1024))

          agg_bytes=$(jq '.runs[] | select(.version=="AggregatedV1") | .max_proof_bytes' "${FOLDING_PERF_OUTPUT}")
          nova_bytes=$(jq '.runs[] | select(.version=="NovaV2") | .max_proof_bytes' "${FOLDING_PERF_OUTPUT}")
          agg_verify=$(jq '.runs[] | select(.version=="AggregatedV1") | .verify_per_proof_ms' "${FOLDING_PERF_OUTPUT}")
          nova_verify=$(jq '.runs[] | select(.version=="NovaV2") | .verify_per_proof_ms' "${FOLDING_PERF_OUTPUT}")

          if (( $(printf '%.0f' "$agg_bytes") > agg_limit_bytes )); then
            echo "::error::AggregatedV1 proof size ${agg_bytes}B exceeds limit ${agg_limit_bytes}B"
            exit 1
          fi
          if (( $(printf '%.0f' "$nova_bytes") > nova_limit_bytes )); then
            echo "::error::NovaV2 proof size ${nova_bytes}B exceeds limit ${nova_limit_bytes}B"
            exit 1
          fi

          python - <<'PY'
import os, sys, json
payload = json.load(open(os.environ['FOLDING_PERF_OUTPUT']))
agg = next(run for run in payload['runs'] if run['version'] == 'AggregatedV1')
limit = float(os.environ['FOLDING_MAX_AGGREGATED_VERIFY_MS'])
sys.exit(0 if agg['verify_per_proof_ms'] <= limit else 1)
PY
          agg_verify_status=$?

          if (( agg_verify_status != 0 )); then
            echo "::error::AggregatedV1 verify time exceeds ${FOLDING_MAX_AGGREGATED_VERIFY_MS}ms"
            exit 1
          fi

          python - <<'PY'
import os, sys, json
payload = json.load(open(os.environ['FOLDING_PERF_OUTPUT']))
nova = next(run for run in payload['runs'] if run['version'] == 'NovaV2')
limit = float(os.environ['FOLDING_MAX_NOVA_VERIFY_MS'])
sys.exit(0 if nova['verify_per_proof_ms'] <= limit else 1)
PY
          nova_verify_status=$?

          if (( nova_verify_status != 0 )); then
            echo "::error::NovaV2 verify time exceeds ${FOLDING_MAX_NOVA_VERIFY_MS}ms"
            exit 1
          fi

      - name: Upload folding artifacts
        uses: actions/upload-artifact@v4
        with:
          name: folding-benchmarks
          path: |
            ${FOLDING_PERF_OUTPUT}
            folding-perf-summary.tsv
